{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "222f61f6",
   "metadata": {},
   "source": [
    "## Mapping the NSD to the FSL for the creation of a SWAT compatiable usersoil dataset\n",
    "\n",
    "this colleciton of scripts takes in the NSD .csv dataset, and the SoilKsatDB database to produce / derive and complete values using a Yggdrasil Decision Forest regresion model to fill in blanks.\n",
    "\n",
    "While the NSD data can be viewed freely via the NSD explorer at https://viewer-nsdr.landcareresearch.co.nz/search, this ommits some key information such as the bulk density and a number of other key values that are essential in the modeling process,\n",
    "as such to replicate this you'll need to contact Manaaki Whenua directly for access to the raw NSD.\n",
    "\n",
    "Many of the key points in this process are derived from the process outlined by Parshotam, 2018 ( https://environment.govt.nz/assets/OIA/Files/20-D-02513_0.pdf ).\n",
    "This includes the mapping for missing NZSC codes found in the FSL but not the NSD, as well as the site selection used in mapping the NZSC codes to NSD samples, aswell as helping in formulate much of the underlying process and logic.\n",
    "\n",
    "After collating and cleaning the bulk of the NSD data, Regression models are derived using the radnom forest models provided by the YDF python package ( https://ydf.readthedocs.io/en/stable/#next-steps ), which builds on from the deprecited tensor flow random forest model. \n",
    "\n",
    "Missing fine earth values are predicted as well as missing rock percentages, follwoing this, the bulk density and avaliable water capacity are predicted, using data from within the NSD.\n",
    "\n",
    "Saturated hyrdaulic conductivity is missing from the NSD, as such the external dataset SoilKsatDB from Gupta, S. et al, 2021 ( https://doi.org/10.5194/essd-13-1593-2021 ) is used to fill in these gaps, by building a similar Regression model based on this data set, it can be applied to the data present in the NSD, initally I inteded to use the equations outlined by Saxton and Rawls, 2006, for the SPAW tool, however given their method I assumed basing it off a more complex dataset thats been made avaliable since would produce better results.\n",
    "\n",
    "Due to limited time, there has been limited analysis and in depth evauluation on the results from these predictions, however especially for the fine earth percentages the derived models had quite low residual error, but for the more complex values such as saturated hydraulic conducitivity and avaliable water capacity, the models produced might not be adequate, however bulk density was on par with the fine earth models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ace405",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Parshotam, 2018. New Zealand SWAT: Deriving a New Zealand-wide soils dataset from the NZ-NSD for use in the Soil and Water Assessment Tool (SWAT). Report prepared for Ministry for the Enviroment. Project: WL18033. Aqualinc Research Limited. https://environment.govt.nz/assets/OIA/Files/20-D-02513_0.pdf\n",
    "\n",
    "- Manaaki Whenua - Landcare Research 2020. National Soils Database (NSD). https://doi.org/10.26060/95m4-cz25\n",
    "\n",
    "- Gupta, S., Hengl, T., Lehmann, P., Bonetti, S., and Or, D.: SoilKsatDB: global database of soil saturated hydraulic conductivity measurements for geoscience applications, Earth Syst. Sci. Data, 13, 1593–1612, https://doi.org/10.5194/essd-13-1593-2021, 2021. \n",
    "\n",
    "- https://ydf.readthedocs.io/en/stable/#next-steps\n",
    "\n",
    "- Saxton, K. E., and W. J. Rawls. “Soil Water Characteristic Estimates by Texture and Organic Matter for Hydrologic Solutions.” Soil Science Society of America Journal, vol. 70, no. 5, 2006, p. 1569, https://doi.org/10.2136/sssaj2005.0117."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4852dcb",
   "metadata": {},
   "source": [
    "## Reading & Preprocessing the NSD data & Gloabl Ksat data\n",
    "\n",
    "this uses the scripts and classes defined in the Useable_NSD_dataset_preprocessing python file. there is alot of choices made in this section any major adjustments to my methods would likely involve changing this section\n",
    "\n",
    "the SoilKsatDB is processed by the Global_kSat_preprocessing script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "515ca6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"Scripts\")\n",
    "\n",
    "## Adding the scripts path to enable calling the scripts from within the \"scripts\" folder\n",
    "## sd_horizon_data and sd_horizon both have very similar data, however each is missing one or two key columns, so I've decided to use both\n",
    "hrzn_1 = \"data/sd_horizon_data.csv\"\n",
    "hrzn_2 = \"data/sd_horizon.csv\"\n",
    "\n",
    "## This is the bulk of the NSD's data, and contains all the actual measurements for each horizon\n",
    "obs_1 = \"data/ob_observation_data.csv\"\n",
    "\n",
    "## Generic descirptions of the soil sites, just used for the gentic description column\n",
    "soil = \"data/sd_soil.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2db6ff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from NSD_dataset_preprocessing import Read_NSD_data\n",
    "\n",
    "## Read_NSD_data is a large function which takes in the provided datasets and cleans, converts and maps all the data into a single table which makes all subsequent steps easier.\n",
    "## a non insignficant amount of the run time is simply loading the ob_observation data\n",
    "\n",
    "NSD_data = Read_NSD_data(hrzn_1, hrzn_2, obs_1, soil)\n",
    "\n",
    "NSD_data.to_csv(\"intermediate_data/NSD_processed_data.csv\", index=False)\n",
    "\n",
    "\n",
    "## The most important variable for soil mapping is total carbon percent.\n",
    "## Filter out rows with missing total carbon percent values.\n",
    "Useable_NSD_data = NSD_data.dropna(subset=[\"total_carbon_percent\"])\n",
    "\n",
    "## There are some negative values in horzion depth, which descrbie leaf liter and top vegetation layers,\n",
    "#  this data is kinda weird to work with and causes problems.\n",
    "\n",
    "Useable_NSD_data = Useable_NSD_data[Useable_NSD_data[\"horizondepth_maxval\"].astype(float) > 0]\n",
    "\n",
    "Useable_NSD_data.to_csv(\"intermediate_data/Useable_NSD_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c53f833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luka/Documents/SWAT/Soil_mapping_refinement/fsl_mapping_org/Scripts/NSD_dataset_preprocessing.py:506: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  df[\"CLR_matrix_hue\"] = df[\"CLR_matrix_hue\"].replace({\"NAN\":\"UNK\"})\n"
     ]
    }
   ],
   "source": [
    "from NSD_dataset_preprocessing import ConvertNSD_To_ML_readable\n",
    "\n",
    "## Takes the dataset produced by the previous function and does numerous operations to split and otherwise modify most of the data columns into fields and binary flags which can be used by a machine learning model.\n",
    "Predictor_data = ConvertNSD_To_ML_readable(Useable_NSD_data)\n",
    "\n",
    "Predictor_data.to_csv(\"intermediate_data/ML_readable_NSD_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d550e4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "529.5228582448085\n",
      "220.6345242686702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luka/Documents/SWAT/Soil_mapping_refinement/fsl_mapping_org/Scripts/Global_kSAT_preprocessing.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sol_ksat_training_set[\"awc\"] = (sol_ksat_training_set[\"w3cld\"] - sol_ksat_training_set[\"w15l2\"])\n",
      "/home/luka/Documents/SWAT/Soil_mapping_refinement/fsl_mapping_org/Scripts/Global_kSAT_preprocessing.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sol_ksat_training_set[\"ksat_lab\"] = ( sol_ksat_training_set[\"ksat_lab\"] * 10 ) / 24.0\n"
     ]
    }
   ],
   "source": [
    "from Global_kSAT_preprocessing import preprocess_ksat_data\n",
    "\n",
    "## Read in the SoilKsatDB data\n",
    "sol_ksat_df = pd.read_csv(\"data/sol_ksat.pnts_horizons.csv\", compression=\"gzip\")\n",
    "\n",
    "## the SoilKsatDB dataset has a handful of super high values, which cuased issues with the model, as such these have been clipped off to give a more generally accurate model.\n",
    "sol_ksat_trainnig_data = preprocess_ksat_data(sol_ksat_df, outlier_threshold=2000)\n",
    "\n",
    "sol_ksat_trainnig_data.to_csv(\"intermediate_data/sol_ksat_trainingdata.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5715cca4",
   "metadata": {},
   "source": [
    "## Predicting NSD Data\n",
    "\n",
    "applies the YDF random forest model onto the dataset created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ae0e24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature DESG_suffix_E is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_M is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_q is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_F is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Train model on 3989 examples\n",
      "Model trained in 0:00:01.686344\n",
      "Feature DESG_suffix_E is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_X is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_M is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_q is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_F is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Train model on 3826 examples\n",
      "Model trained in 0:00:01.650365\n",
      "Feature DESG_suffix_E is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_X is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_M is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_q is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_F is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Train model on 3921 examples\n",
      "Model trained in 0:00:01.543319\n",
      "Feature DESG_suffix_E is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_M is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_q is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_F is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Train model on 3962 examples\n",
      "Model trained in 0:00:00.454639\n",
      "Feature DESG_suffix_E is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_X is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_j is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_d is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_M is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_q is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_F is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_5 is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Train model on 1963 examples\n",
      "Model trained in 0:00:00.665901\n",
      "Feature DESG_suffix_E is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_X is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_j is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_d is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_M is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_q is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_F is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Feature DESG_suffix_5 is a NUMERICAL feature with all values recorded in the data spec set to the same value. The feature will likely not be useful during model training.\n",
      "Train model on 1934 examples\n",
      "Model trained in 0:00:00.447075\n",
      "Train model on 6638 examples\n",
      "Model trained in 0:00:00.199451\n"
     ]
    }
   ],
   "source": [
    "from NSD_Predictor import Predict_data\n",
    "\n",
    "## based on the NSD and SoilKsatDB data that has been preprocessed\n",
    "# this returns the data in the machine learning readable format.\n",
    "NSD_predicted = Predict_data(Predictor_data, sol_ksat_trainnig_data)\n",
    "\n",
    "NSD_predicted.to_csv(\"intermediate_data/NSD_Horizions_Predicted.csv\")\n",
    "\n",
    "## Fill in the missing values of the columns of interest in the Usable NSD dataset.\n",
    "\n",
    "\n",
    "Useable_NSD_data[\"clay_percent\"] = Useable_NSD_data[\"clay_percent\"].fillna(NSD_predicted[\"clay_percent\"])\n",
    "Useable_NSD_data[\"silt_percent\"] = Useable_NSD_data['silt_percent'].fillna(NSD_predicted[\"silt_percent\"])\n",
    "Useable_NSD_data[\"sand_percent\"] = Useable_NSD_data['sand_percent'].fillna(NSD_predicted[\"sand_percent\"])\n",
    "\n",
    "Useable_NSD_data[\"rock_fragment_percent\"] = Useable_NSD_data['rock_fragment_percent'].fillna(NSD_predicted[\"rock_fragment_percent\"])\n",
    "\n",
    "Useable_NSD_data[\"whole_bulk_density\"] = Useable_NSD_data['whole_bulk_density'].fillna(NSD_predicted[\"whole_bulk_density\"])\n",
    "Useable_NSD_data[\"available_water_capacity\"] = Useable_NSD_data['available_water_capacity'].fillna(NSD_predicted[\"available_water_capacity\"])\n",
    "\n",
    "Useable_NSD_data[\"ksat\"] = NSD_predicted[\"ksat\"]\n",
    "\n",
    "Useable_NSD_data.to_csv(\"intermediate_data/NSD_Horizions_Usable_filled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c9745",
   "metadata": {},
   "source": [
    "## Map the Predicted NSD results the FSL \n",
    "this step uses the tables derived by Parshotam, 2018\n",
    "\n",
    "This is the last step in the process and maps the completed / predicted NSD data, to a corresponding FSL mapping, as is this makes a national scale Soil map based on unique NZSC codes\n",
    "\n",
    "This process could be adjusted to make a site specific mapping, if more apporiate NSD samples are identified for a specific site, \n",
    "\n",
    "FSL layer has a lot of assumptions and a national scale dataset needs to make some consecisions \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "898c22fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import geopandas as gd\n",
    "\n",
    "fsl_shape_file = r\"lris-fsl-north-island-v11-all-attributes-SHP/fsl-north-island-v11-all-attributes.shp\"\n",
    "\n",
    "## These are codes that exist in FSL but not in NSD,\n",
    "# so we need to map them to the closest equivalent, this was derived in the following paper https://environment.govt.nz/assets/OIA/Files/20-D-02513_0.pdf\n",
    "\n",
    "missing_code_lookup = pd.read_csv(\"data/MissingNZSC_Replacements.csv\")\n",
    "\n",
    "## This is the table that associates each NZSC code, with a NSD site, this is based again on the work done here https://environment.govt.nz/assets/OIA/Files/20-D-02513_0.pdf\n",
    "## Because this process uses the NZSC code to match sites, any adjustment to this process, \n",
    "# say if muitple sites want to be used for slightly different FSL regions which have matching NZSC codes, will need a change in the following logic\n",
    "\n",
    "nzsc_nsd_lookup_df = pd.read_csv(\"data/FSL_NZSC_NSD_SITE_MAP.csv\")\n",
    "\n",
    "## Split the NZSC codes and grab the short hand version\n",
    "Useable_NSD_data[\"classifier_nzsc_short\"] = Useable_NSD_data[\"classifier_nzsc\"].str.split().str[0]\n",
    "nsd_nzsc_codes = Useable_NSD_data[\"classifier_nzsc_short\"].unique().tolist()\n",
    "\n",
    "## Read in the FSL layer, apply the missing NZSC code map, and map in the assigned NSD site codes.\n",
    "## This could also use the Main soil type field as a mapping, however this will need a unique NSD Site code map.\n",
    "\n",
    "fsl_data = gd.read_file(fsl_shape_file)\n",
    "\n",
    "fsl_data[\"NSD_NZSC_CODE_MAP\"] = fsl_data[\"DOMNZSC\"].apply(lambda x: x if x in nsd_nzsc_codes else (missing_code_lookup[x] if x in missing_code_lookup else \"MISSING\"))\n",
    "fsl_data[\"NSD_SITE_CODE\"] = fsl_data[\"NSD_NZSC_CODE_MAP\"].apply(lambda x: nzsc_nsd_lookup_df.loc[nzsc_nsd_lookup_df[\"NZSC_CODE\"] == x, \"NSD_SITE_CODE\"].values[0] if x in nzsc_nsd_lookup_df[\"NZSC_CODE\"].values else \"MISSING\")\n",
    "\n",
    "## find the NSD layers that are used in the site lookup table\n",
    "sig_nsd_layers = Useable_NSD_data[Useable_NSD_data[\"site_identifier\"].isin(nzsc_nsd_lookup_df[\"NSD_SITE_CODE\"].unique().tolist())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333ea0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "## Create the Headers for the usersoil dataset. \n",
    "\n",
    "## Find the number of sites, and the maximum numnber of layers of any of them.\n",
    "SITE_NLAYERS = sig_nsd_layers[\"site_identifier\"].value_counts()\n",
    "# MAX_LAYERS = int(sig_nsd_layers[\"horizonnumber\"].astype(float).max())\n",
    "MAX_LAYERS = 10\n",
    "print(MAX_LAYERS)\n",
    "## Two sets of headers, one which is per site, and one which is per layer per site.\n",
    "\n",
    "primary_headers = [\"OBJECTID\",\"SITE_ID_SB\",\"SITE_ID\",\"MUID\",\"SEQN\",\"SNAM\",\"S5ID\",\"CMPPCT\",\"NLAYERS\",\"HYDGRP\",\"SOL_ZMX\",\"ANION_EXCL\",\"SOL_CRK\",\"TEXTURE\"]\n",
    "layer_headers = [\"LAYER_ID#\", \"SOL_Z#\", \"SOL_BD#\", \"SOL_AWC#\", \"SOL_K#\", \"SOL_CBN#\", \"CLAY#\", \"SILT#\", \"SAND#\", \"ROCK#\", \"SOL_ALB#\",\"USLE_K#\",\"SOL_EC#\"]\n",
    "\n",
    "addon_headers = [\"SOL_CAL#\", \"SOL_PH#\"] ## Not activated but these columns still need to be in the usersoil file, otherwise it causes issues.\n",
    "## Given the maximum number of layers, populate the layer headers acordingly.\n",
    "\n",
    "layer_headers_renamed = [header.replace(\"#\", str(i)) for i in range(1, MAX_LAYERS+1) for header in layer_headers]\n",
    "addon_headers = [header.replace(\"#\", str(i)) for i in range(1, MAX_LAYERS+1) for header in addon_headers] ## Not activated but these columns still need to be in the usersoil file, otherwise it causes issues.\n",
    "\n",
    "## make the data frame\n",
    "\n",
    "usersoil_headers = primary_headers + layer_headers_renamed + addon_headers\n",
    "usersoil = pd.DataFrame(columns=usersoil_headers, index=range(len(SITE_NLAYERS)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dd53d7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLAYERS\n",
      "5     44\n",
      "7     29\n",
      "6     22\n",
      "4     18\n",
      "8     12\n",
      "3     11\n",
      "10     7\n",
      "9      4\n",
      "2      1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28259/3287003887.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sig_nsd_layers[\"horizonnumber\"] = sig_nsd_layers[\"horizonnumber\"].astype(float).astype(int)\n"
     ]
    }
   ],
   "source": [
    "## Map the Per Site Columns to target NSD sites ( uses the NSD_data data not the sa_site data )\n",
    "\n",
    "## Need to convert the horizon number form a string float to a interger to prevent issues later on ( there will be a b)\n",
    "sig_nsd_layers[\"horizonnumber\"] = sig_nsd_layers[\"horizonnumber\"].astype(float).astype(int)\n",
    "\n",
    "usersoil[\"SITE_ID\"] = sig_nsd_layers[\"sd_soil_id\"].unique().tolist()  \n",
    "\n",
    "sig_ob_site_id = sig_nsd_layers[[\"sd_soil_id\", \"site_identifier\"]].set_index(\"sd_soil_id\").to_dict()[\"site_identifier\"]\n",
    "sig_ob_nlayers = sig_nsd_layers[[\"sd_soil_id\", \"horizonnumber\"]].groupby(\"sd_soil_id\").max().to_dict()[\"horizonnumber\"]\n",
    "sig_ob_class = sig_nsd_layers[[\"sd_soil_id\", \"classifier_nzsc\"]].set_index(\"sd_soil_id\").to_dict()[\"classifier_nzsc\"]\n",
    "# sig_ob_max_depth = sig_nsd_layers[[\"sd_soil_id\", \"horizondepth_maxval\"]].groupby(\"sd_soil_id\").max().to_dict()[\"horizondepth_maxval\"]\n",
    "\n",
    "\n",
    "usersoil[\"SITE_ID_SB\"] = usersoil[\"SITE_ID\"].map(sig_ob_site_id)\n",
    "usersoil[\"NLAYERS\"] = (usersoil[\"SITE_ID\"].map(sig_ob_nlayers).astype(float)).astype(int)\n",
    "\n",
    "## The QSWAT plugin I'm using for SWAT cannot handle more than 10 layers, while not common, some sites contain more than 10, in one case the best / only choice has 13 layers,\n",
    "    # clipping off at 10 is a crude solution and potentially \"disolving\" some layers based on comminality would probably be better, but this works.\n",
    "usersoil[\"NLAYERS\"] = usersoil[\"NLAYERS\"].apply(lambda x: x if x <= 10 else 10)\n",
    "print(usersoil[\"NLAYERS\"].value_counts())\n",
    "\n",
    "## Sets the SNAM as the NZSC code and strips off the long hand name\n",
    "usersoil[\"SNAM\"] = usersoil[\"SITE_ID\"].map(sig_ob_class)\n",
    "usersoil[\"SNAM\"] = usersoil[\"SNAM\"].str.split().str[0]\n",
    "\n",
    "\n",
    "\n",
    "## Gets the max depth and converts it to mm\n",
    "# usersoil[\"SOL_ZMX\"] = usersoil[\"SITE_ID\"].map(sig_ob_max_depth)\n",
    "# usersoil[\"SOL_ZMX\"] = usersoil[\"SOL_ZMX\"].astype(float) * 10\n",
    "\n",
    "usersoil[\"OBJECTID\"] = range(1, len(usersoil)+1)\n",
    "\n",
    "usersoil.to_csv(\"nzsc_usersoil.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "78149027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28259/2039535475.py:93: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  usersoil[\"ANION_EXCL\"] = usersoil[\"ANION_EXCL\"].fillna(0.5)\n",
      "/tmp/ipykernel_28259/2039535475.py:94: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  usersoil[\"SOL_CRK\"] = usersoil[\"SOL_CRK\"].fillna(0.5)\n",
      "/tmp/ipykernel_28259/2039535475.py:97: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  usersoil = usersoil.fillna(0)\n"
     ]
    }
   ],
   "source": [
    "## I've run into far to many issues in this section given what it's doing. \n",
    "# if it doesn't work try restarting the kernel and running again ( it's solved too many of the issues )\n",
    "\n",
    "\n",
    "from calc_usle_k import calc_usle_k\n",
    "import numpy as np\n",
    "\n",
    "## Create a dict of all the horizion ids, based on the site id and horizion number\n",
    "sig_ob_layer_ids = sig_nsd_layers[[\"sd_horizon_id\", \"horizonnumber\", \"sd_soil_id\"]].set_index([\"sd_soil_id\", \"horizonnumber\"]).to_dict()[\"sd_horizon_id\"]\n",
    "# print(sig_ob_layer_ids)\n",
    "## Becuase of the preprocessing steps, this doesn't need to handle any errors in the data, such as soil layers above depth 0, however this should be kept in mind if issues arise.\n",
    "\n",
    "##  Its not best practice to iterate over pandas rows, but this isn't a high frequency operation.\n",
    "\n",
    "## Becuase, there are often layers, either above ground ( leaf litter etc ), that I've ommited, the horizonnumber value, doesn't always line up with what is present\n",
    "## This accounts for that and adjusts the data accordingly before mapping the actual values in.\n",
    "for row in usersoil.itertuples():\n",
    "    site_id = row.SITE_ID\n",
    "    missing_layers = 0\n",
    "    i = 1\n",
    "    \n",
    "    while i < row.NLAYERS + 1:\n",
    "        if (site_id, i) in sig_ob_layer_ids:\n",
    "            usersoil.at[row.Index, f\"LAYER_ID{i-missing_layers}\"] = sig_ob_layer_ids.get((site_id, i), None)\n",
    "            # print(f\"Mapping layer {i} for site {site_id} to horizon id {usersoil.at[row.Index, f'LAYER_ID{i}']}, for Layer {i-missing_layers}\")\n",
    "            i += 1\n",
    "        else:\n",
    "            missing_layers += 1\n",
    "            i += 1\n",
    "            # print(f\"Skipping missing or shallow layer {i} for site {site_id}\")\n",
    "    \n",
    "    usersoil.at[row.Index, \"NLAYERS\"] = row.NLAYERS - missing_layers\n",
    "    \n",
    "\n",
    "## Maps the data both predicted and measured from the NSD into the usersoil data    \n",
    "for row in usersoil.itertuples():\n",
    "    \n",
    "    site_id = row.SITE_ID\n",
    "    \n",
    "    for i in range(1, row.NLAYERS + 1):\n",
    "        layer_id = getattr(row, f\"LAYER_ID{i}\")\n",
    "        # print(layer_id)\n",
    "        if pd.notna(layer_id):\n",
    "            \n",
    "            ## Grab the row based on the horizon id, and reset the index so it can be accessed via .loc[0]            \n",
    "            target_row = sig_nsd_layers.loc[sig_nsd_layers['sd_horizon_id'] == layer_id].reset_index()\n",
    "            \n",
    "            rock_frag = float(target_row['rock_fragment_percent'].loc[0])           \n",
    "            clay = float(target_row[\"clay_percent\"].loc[0])\n",
    "            silt = float(target_row[\"silt_percent\"].loc[0])\n",
    "            sand = float(target_row[\"sand_percent\"].loc[0])\n",
    "            \n",
    "            ### Removes any errors in the fine earth values, as to ensure they sum to 100%\n",
    "            fine_earth_frac = (clay+silt+sand)/100.0\n",
    "            clay = clay / fine_earth_frac\n",
    "            silt = silt / fine_earth_frac\n",
    "            sand = sand / fine_earth_frac\n",
    "            \n",
    "            ## The SWAT process needs the rock frag and fine earth to sum to 100, this adjusts the fine earth to take into account the rock percentage\n",
    "            rock_frag_frac = 1 - (rock_frag)/100\n",
    "            clay = rock_frag_frac * clay\n",
    "            silt = rock_frag_frac * silt\n",
    "            sand = rock_frag_frac * sand\n",
    "            \n",
    "            ## Write the commupted values into the usersoil at the give column and row\n",
    "            usersoil.at[row.Index, f\"SOL_Z{i}\"] = float(target_row['horizondepth_maxval'].loc[0]) * 10\n",
    "            if i == row.NLAYERS:\n",
    "                usersoil.at[row.Index, \"SOL_ZMX\"] = float(target_row['horizondepth_maxval'].loc[0]) * 10\n",
    "            \n",
    "            usersoil.at[row.Index, f\"ROCK{i}\"] = rock_frag\n",
    "            usersoil.at[row.Index, f\"CLAY{i}\"] = clay\n",
    "            usersoil.at[row.Index, f\"SILT{i}\"] = silt\n",
    "            usersoil.at[row.Index, f\"SAND{i}\"] = sand\n",
    "                        \n",
    "            usersoil.at[row.Index, f\"SOL_BD{i}\"] = float(target_row[\"whole_bulk_density\"].loc[0])\n",
    "            usersoil.at[row.Index, f\"SOL_AWC{i}\"] = float(target_row[\"available_water_capacity\"].loc[0])\n",
    "            usersoil.at[row.Index, f\"SOL_CBN{i}\"] = float(target_row[\"total_carbon_percent\"].loc[0])\n",
    "            usersoil.at[row.Index, f\"SOL_K{i}\"] = abs(float(target_row[\"ksat\"].loc[0]))     ## This might be a bad idea, but negative Ksat values don't make sense.\n",
    "\n",
    "            usersoil.at[row.Index, f\"SOL_ALB{i}\"] = float(target_row['CLR_alb'].loc[0])\n",
    "\n",
    "            ## Calculate the Usle K value\n",
    "            usersoil.at[row.Index, f\"USLE_K{i}\"] = calc_usle_k(usersoil.at[row.Index, f\"SOL_CBN{i}\"], usersoil.at[row.Index, f\"SAND{i}\"], usersoil.at[row.Index, f\"SILT{i}\"], usersoil.at[row.Index, f\"CLAY{i}\"])\n",
    "            \n",
    "\n",
    "## Add a blank row at the start for water values\n",
    "usersoil.loc[len(usersoil)] = None\n",
    "usersoil = usersoil.shift()\n",
    "usersoil.at[0, \"SNAM\"] = \"WTR\"\n",
    " \n",
    "## Fill in some of the unused columns with default values\n",
    "usersoil[\"HYDGRP\"] = usersoil[\"HYDGRP\"].fillna(\"C\")         ## Not required, filled with C as a placeholder\n",
    "usersoil[\"ANION_EXCL\"] = usersoil[\"ANION_EXCL\"].fillna(0.5)\n",
    "usersoil[\"SOL_CRK\"] = usersoil[\"SOL_CRK\"].fillna(0.5)\n",
    "usersoil[\"TEXTURE\"] = usersoil[\"TEXTURE\"].fillna(\"UNK\")       ## Not required \n",
    "\n",
    "usersoil = usersoil.fillna(0)\n",
    "\n",
    "### Drop the columns containg the various id's, doesn't drop the SB site identifier code tho, so each layer can still be looked up in the NSD for validation\n",
    "usersoil = usersoil[usersoil.columns.drop(list(usersoil.filter(regex='LAYER_ID')))]\n",
    "usersoil = usersoil[usersoil.columns.drop(\"SITE_ID\")]\n",
    "\n",
    "\n",
    "usersoil = usersoil.drop_duplicates(subset=\"SNAM\")\n",
    "\n",
    "usersoil.to_csv(\"nzsc_usersoil.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3419a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MachineLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
